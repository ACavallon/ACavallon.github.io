
<p>-This page is part of the <a href="RAI_landing_page.html"> Responsible AI </a> series.</p>
<ol>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#listofbias">List of bias</a><ul>
<li><a href="#cognitivebias">Cognitive Bias</a>    </li>
<li><a href="#technicalbias">Technical Bias</a></li>
</ul>
</li>
<li><a href="#biasmetrics">Bias Metrics</a></li>
</ol>
<h1 id="introduction-a-id-introduction-a-">Introduction <a id="introduction"></a></h1>
<p>Bias and fairness are critical considerations when it comes to artificial intelligence (AI) systems. In the context of AI, bias refers to the systematic errors or deviations from an accurate or fair outcome that can occur in the development and deployment of AI algorithms. </p>
<h3 id="bias">Bias</h3>
<p>Bias can manifest in different forms, with statistical bias and ethics bias being two distinct aspects:</p>
<ul>
<li><strong>Statistical bias</strong>, often referred to as algorithmic bias, is a type of bias that arises from the data used to train machine learning models. If the training data contains inherent biases or reflects historical inequalities, the resulting AI systems can perpetuate and amplify those biases. For example, if a hiring algorithm is trained on biased historical hiring data, it may inadvertently discriminate against certain demographic groups when making hiring recommendations.</li>
<li><strong>Ethics bias</strong> refers to the biases that arise from the ethical judgments and values embedded in the design, development, and decision-making processes of AI systems. This type of bias can emerge when the objectives or criteria used to optimize AI algorithms reflect certain societal or individual biases. For instance, if an AI system is designed to maximize profit without considering potential social or environmental impacts, it may prioritize short-term financial gains over long-term societal well-being.</li>
</ul>
<p>Recognizing and addressing both statistical bias and ethics bias are essential for ensuring fairness in AI systems. Mitigating statistical bias requires careful data preprocessing, ensuring representative and diverse training datasets, and employing techniques such as bias correction and fairness-aware learning. Ethics bias, on the other hand, calls for critical reflection, ethical frameworks, and stakeholder engagement throughout the AI development lifecycle to identify and challenge the underlying assumptions and values shaping the system&#39;s behavior.</p>
<p>Achieving fairness in AI is a multifaceted endeavor that requires a holistic approach. It involves a combination of technical solutions, regulatory measures, and ethical considerations to promote transparency, accountability, and inclusivity. By addressing statistical bias and ethics bias, we can strive towards AI systems that treat individuals equitably, avoid discrimination, and contribute to a more just and unbiased society.</p>
<h1 id="list-of-bias-a-id-listofbias-a-">List of Bias  <a id="listofbias"></a></h1>
<ul>
<li><a href="https://towardsdatascience.com/an-unbiased-guide-to-bias-in-ai-3841c2b36165">An “Unbiased” Guide to Bias in AI</a>. Differences between statistical bias and ethics bias. Ethical bias by proxy. Ethical bias through historical biased decisions. Ethical bias through an inappropriate choice of target.</li>
</ul>
<p><img src=":/5943ce0a964d4781b7860b7e3b44243e" alt="6b3d7b0afe55ea7a2c0b356768e65599.png">
More in details
<img src=":/65fbd1e656fd4b0a8558f9817ab214ee" alt="4a4762a4cc72f87fdc7fe5c01c3d0114.png"></p>
<ul>
<li><a href="https://towardsdatascience.com/biased-ai-a-look-under-the-hood-5d0a41968f16">Biased AI, a Look Under the Hood</a></li>
<li><a href="https://arxiv.org/pdf/1901.10002.pdf">A Framework for Understanding Sources of Harm throughout the Machine Learning Life Cycle</a></li>
</ul>
<h2 id="cognitive-bias-a-id-cognitivebias-a-">Cognitive Bias  <a id="cognitivebias"></a></h2>
<h3 id="inductive-bias">Inductive Bias</h3>
<ul>
<li><a href="https://mindfulmodeler.substack.com/p/from-theory-to-practice-inductive?utm_source=profile&amp;utm_medium=reader2">Inductive Bias Series, part 1 - From Theory to Practice: Inductive Biases in Machine Learning</a></li>
<li><a href="https://mindfulmodeler.substack.com/p/take-the-inductive-leap?utm_source=profile&amp;utm_medium=reader2">Inductive Bias Series, part 2 - Take the inductive leap</a></li>
<li><a href="https://mindfulmodeler.substack.com/p/ignore-inductive-biases-at-your-own?utm_source=profile&amp;utm_medium=reader2">Inductive Bias Series, part 3 - Ignore inductive biases at your own peril</a></li>
<li><a href="https://mindfulmodeler.substack.com/p/inductive-biases-of-the-random-forest?utm_source=profile&amp;utm_medium=reader2">Inductive Bias Series, part 4 - Inductive biases of the Random Forest and their consequences</a></li>
<li><a href="https://mindfulmodeler.substack.com/p/statistical-modeling-seen-through?utm_source=profile&amp;utm_medium=reader2">Inductive Bias Series, part 6 - Statistical modeling seen through inductive biases</a></li>
</ul>
<h2 id="technical-bias-a-id-technicalbias-a-">Technical bias  <a id="technicalbias"></a></h2>
<h3 id="omitted-variable-bias">Omitted Variable Bias</h3>
<ul>
<li><a href="https://towardsdatascience.com/omitted-variable-bias-and-what-can-we-do-about-it-344ac1477699">Omitted Variable Bias And How To Deal With It</a></li>
</ul>
<h2 id="bias-variance-tradeoff">Bias-variance tradeoff</h2>
<ul>
<li><a href="https://towardsdatascience.com/mathematical-understanding-of-bias-variance-tradeoff-9366dbc8439a">Mathematical Understanding of Bias Variance Tradeoff</a></li>
<li><a href="https://towardsdatascience.com/thorough-examination-of-bias-and-variance-in-the-linear-regression-4b08bc2bb8da">Thorough examination of bias and variance in the linear regression</a></li>
<li><a href="https://towardsdatascience.com/understanding-bias-variance-trade-off-from-a-bayesian-perspective-6c31645fcaa9">Understanding Bias-Variance Trade-off from a Bayesian Perspective</a></li>
<li><a href="https://towardsdatascience.com/is-there-always-a-tradeoff-between-bias-and-variance-5ca44398a552">Is There Always a Tradeoff Between Bias and Variance?</a></li>
<li><a href="https://www.youtube.com/watch?v=zrEyxfl2-a8&amp;list=PLD63A284B7615313A&amp;index=10&amp;ab_channel=caltech">Caltech - Lecture 08 - Bias-Variance Tradeoff</a> [Youtube]</li>
<li><a href="https://towardsdatascience.com/cognitive-biases-in-data-science-the-category-size-bias-8dbd851608c3">Cognitive Biases in Data Science: The Category-Size Bias</a></li>
</ul>
<hr>
<h1 id="bias-metrics-a-id-biasmetrics-a-">Bias metrics   <a id="biasmetrics"></a></h1>
<p>Here is a list of metrics to do bias detection and to force the system to be &quot;fair&quot; according to these metrics.</p>
<ul>
<li><a href="#equalperformance">Equal performance</a><ul>
<li><a href="#equalizedodds">Equalized odds</a></li>
<li><a href="#predictiveparity">Predictive parity</a></li>
</ul>
</li>
<li><a href="#demographicparity">Demographic Parity</a></li>
<li><a href="#treatmentequality">Treatment Equality</a></li>
</ul>
<h2 id="equal-performance-a-id-equalperformance-a-">Equal performance <a id="equalperformance"></a></h2>
<p><strong><a href="https://oecd.ai/en/catalogue/metrics/equal-performance">Equal performance</a></strong> refers to the assurance that a model is equally accurate for patients in the protected and non-protected groups. Equal performance has 3 commonly discussed types: </p>
<ul>
<li>equal sensitivity (also known as <strong>equal opportunity</strong>), </li>
<li>equal sensitivity and specificity, i.e. equal true positive rate and equal false positive rate (also known as <strong>equalized odds</strong>), </li>
<li>equal positive predictive value (commonly referred to as <strong>predictive parity</strong>). </li>
</ul>
<h3 id="equalized-odds-a-id-equalizedodds-a-">Equalized Odds <a id="equalizedodds"></a></h3>
<p><img src=":/387b806a42904004a028e7e2776a2587" alt="489f824bcadd2b751c3469d9907ac740.png"> 
where A is the sensitive feature and R is a binary output {+,-}. The concept was originally defined for binary-valued Y, but in 2017, Woodworth et al. generalized the concept further for multiple classes.</p>
<h4 id="use-cases">Use Cases</h4>
<ul>
<li>Criminal justice systems to ensure equal false positive and true positive rates across different races.</li>
<li>Medical diagnosis systems to ensure equal error rates across genders.<h4 id="limitations">Limitations</h4>
</li>
<li>Difficult to achieve in practice, as it requires balancing multiple rates</li>
<li>May conflict with other fairness criteria and overall accuracy</li>
</ul>
<h4 id="sources">Sources</h4>
<ul>
<li><a href="https://mlu-explain.github.io/equality-of-odds/?utm_source=substack&amp;utm_medium=email">Equality of Odds</a></li>
</ul>
<h3 id="predictive-parity-a-id-predictiveparity-a-">Predictive Parity <a id="predictiveparity"></a></h3>
<p><img src=":/91a4337206994a938a528360eb087c4e" alt="ca2d7eefa6f3aec747ffc436ad245f8a.png">
where A is the sensitive feature and R is a binary output {+,-}. Predictive parity ensures that the predicted positive outcome has the same precision across different groups.</p>
<h4 id="use-cases">Use Cases</h4>
<ul>
<li>Loan default predictions to ensure equal precision across different demographic groups</li>
<li>Healthcare treatment recommendations to ensure equal accuracy across different patient groups<h4 id="limitations">Limitations</h4>
</li>
<li>May not address underlying disparities in data distribution</li>
<li>Can conflict with other fairness metrics like equalized odds</li>
</ul>
<h2 id="demographic-parity-a-id-demographicparity-a-">Demographic Parity <a id="demographicparity"></a></h2>
<p><strong>Demographic parity</strong> or <strong>Statistical parity</strong> (also referred as acceptance rate parity or benchmarking) refers to the property of a classifier where he subjects in the protected and unprotected groups have equal probability of being assigned to the positive predicted class. This metrics consider only the predicted outcome, not the actual outcome.
<img src=":/92b75a4bb14c44e19b18afea715f6687" alt="38e06259af3839cfbad5acd87455533e.png">
where A is the sensitive feature and R is a binary output {+,-}.</p>
<h3 id="use-cases">Use Cases</h3>
<ul>
<li>Hiring algorithms to ensure equal hiring rates across genders</li>
<li>Loan approval systems to provide equal approval rates across different ethnicities<h3 id="limitations">Limitations</h3>
</li>
<li>May not account for differences in group qualifications or characteristics</li>
<li>Can lead to reverse discrimination if strictly enforced</li>
</ul>
<h2 id="treatment-equality-a-id-treatmentequality-a-">Treatment equality <a id="treatmentequality"></a></h2>
<p><strong>Treatment equality</strong> focuses on balancing the ratio of false positives to false negatives across different groups. A classifier satisfies this definition if the subjects in the protected and unprotected groups have an equal ratio of FN and FP, satisfying the formula:
<img src=":/531734831144414aa0da644c01e225c2" alt="aa3c9fe06e5a0d8a09331524f7998a2a.png"></p>
<h4 id="use-cases">Use Cases</h4>
<ul>
<li>Predictive policing to balance false arrest rates and failure to arrest rates across different communities</li>
<li>Fraud detection systems to balance the rates of false alarms and missed frauds across different customer segments.<h4 id="limitations">Limitations</h4>
</li>
<li>Complex to calculate and interpret</li>
<li>May lead to trade-offs with overall model accuracy</li>
</ul>
<h1 id="mitigation-of-bias">Mitigation of bias</h1>
<ul>
<li><strong>Fairness through unawareness (FTU)</strong> <em>(or anti-classification)</em>. There is a very intuitive approach to fairness, which usually goes under the name of fairness through unawareness (FTU), or blindness, that prescribes not to explicitly employ sensitive features when making (automated) decisions. This is effectively a notion of individual fairness, since two individuals differing only for the value of their sensitive attributes would receive the same outcome. However, in general, FTU is subject to several drawbacks, the main being that it does not take into account possible correlations between sensitive attributes and non-sensitive attributes employed in the decision-making process. For example, an agent with the (malignant) intention to discriminate on the basis of gender could introduce in the model a proxy variable for gender (i.e. a variable highly correlated with gender) and effectively using gender information while at the same time being compliant to the FTU prescription.</li>
</ul>
<ul>
<li><a href="https://towardsdatascience.com/how-to-remove-bias-in-machine-learning-training-data-d54967729f88">How to Remove Bias in Machine Learning Training Data</a>
<img src=":/7c3b8d9967ef4ecf9f201d643ecbe8d4" alt="biases_datasets.jpg"></li>
</ul>
<h2 id="bias-mitigation-in-classification">Bias mitigation in classification</h2>
<ul>
<li><a href="https://www.linkedin.com/pulse/handling-data-bias-multi-class-text-classification-bijoy-boban/">Handling Data Bias in multi-class text classification</a></li>
<li><a href="https://medium.com/@engineering_holistic_ai/bias-metrics-for-regression-multi-class-models-24d14ecf29c4">Bias Metrics for Regression &amp; Multi-class Models</a></li>
</ul>
<h1 id="fairness">Fairness</h1>
<ul>
<li><p><a href="https://towardsdatascience.com/analysing-fairness-in-machine-learning-with-python-96a9ab0d0705">Analysing Fairness in Machine Learning (with Python)</a>
<img src=":/6ee328a3d1f34d739551f9b964649904" alt="13e4c1458a45afa587fd8f63b29632bf.png"></p>
</li>
<li><p><a href="https://arxiv.org/pdf/2102.08453.pdf">Towards the Right Kind of Fairness in AI</a> [ArXiv]</p>
</li>
</ul>
<h1 id="libraries">Libraries</h1>
<h2 id="1-fairlearn">1. Fairlearn</h2>
<p>Fairlearn is an open-source Python library developed by Microsoft. It provides tools for assessing and mitigating unfairness in machine learning models. Fairlearn offers both fairness metrics and algorithms for reducing bias. It also includes visualization tools to help interpret fairness metrics and mitigation results.</p>
<ul>
<li><a href="https://fairlearn.org/">Homepage</a></li>
<li><a href="https://fairlearn.org/v0.10/user_guide/fairness_in_machine_learning.html">Fairness in machine learning</a></li>
<li><p><a href="https://github.com/fairlearn/fairlearn">Fairlearn&#39;s GitHub</a></p>
</li>
<li><p>AIF360
AIF360 (AI Fairness 360) is an open-source toolkit developed by IBM. It aims to help detect and mitigate bias in AI models through a comprehensive set of fairness metrics and bias mitigation algorithms. It’s known for its extensive documentation and tutorials to guide you through the process of fairness assessment.</p>
</li>
<li><p>Themis-ML
Themis-ML is an open-source library focused on fairness-aware machine learning. It provides tools for implementing and evaluating fairness metrics and algorithms, particularly for binary classification tasks. It’s also designed to integrate easily with existing machine learning workflows using scikit-learn.</p>
</li>
<li><p>Fairness Indicators
Fairness Indicators is a library developed by Google to help assess the fairness of machine learning models. It provides tools for evaluating fairness metrics and visualizing disparities across different demographic groups. It’s designed to work seamlessly with TensorFlow Extended and TensorFlow Model Analysis.</p>
</li>
<li><p>FairComp
FairComp is an open-source library for comparing different fairness interventions and metrics. It provides a standardized framework for evaluating the trade-offs between fairness and accuracy. It also enables benchmarking of models against various fairness metrics.</p>
</li>
</ul>
<h1 id="tools">Tools</h1>
<ul>
<li><a href="https://kayaconnect.org/course/info.php?id=5112">Gender Equitable AI Toolkit</a>. An iterative e-learning toolkit designed by the NGO sector for the NGO sector to enable the implementation of gender equitable AI / ML solutions</li>
<li><a href="https://dssg.github.io/aequitas/">Aequitas</a>. Aequitas is an open-source bias audit toolkit for machine learning developers, analysts, and policymakers to audit machine learning models for discrimination and bias, and to make informed and equitable decisions around developing and deploying predictive risk-assessment tools.</li>
</ul>
<h1 id="llms">LLMs</h1>
<p>The specific page for RAI in LLMs can be found here <a href=":/3436b26c264c4eb99dd9e17a26bc789a">RAI LLM</a>.</p>
<h1 id="emerging-topics">Emerging topics</h1>
<ul>
<li>Fairness-Aware AI Development: Increasing focus on integrating fairness considerations throughout the AI development lifecycle, from data collection to model deployment</li>
<li>Explainable AI (XAI): Development of techniques to make AI models more interpretable and transparent, helping stakeholders understand how decisions are made and identify potential biases</li>
<li>Intersectional Fairness: Research on addressing intersectional biases that affect individuals belonging to multiple marginalized groups, going beyond single-axis fairness metrics</li>
<li>Algorithmic Auditing: Growth of third-party auditing practices to assess and certify the fairness of AI systems, providing external validation and accountability</li>
<li>Collaborative Fairness: Collaboration between AI developers, social scientists, and ethicists to create more holistic and socially aware AI systems</li>
<li>Bias Mitigation Technologies: Advancements in bias mitigation techniques, including new algorithms and tools that reduce bias while maintaining model performance</li>
</ul>
<h1 id="other-readings">Other readings</h1>
<ul>
<li><a href="https://towardsdatascience.com/healthcare-is-inherently-biased-b60bf00d4af7">Healthcare Data Is Inherently Biased</a></li>
<li><a href="https://superwise.ai/blog/gentle-introduction-ml-fairness-metrics/">A gentle introduction to ML fairness metrics</a></li>
<li><a href="https://hbr.org/2020/10/ai-fairness-isnt-just-an-ethical-issue">AI Fairness Isn’t Just an Ethical Issue</a></li>
<li><a href="https://towardsdatascience.com/to-guarantee-impartial-ai-decisions-lady-justice-needs-to-blink-2992167b2591">To Guarantee Impartial AI Decisions, Lady Justice Needs to Blink</a></li>
</ul>
<h1 id="to-be-controlled">To be controlled</h1>
<p><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3547922">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3547922</a>
<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3792772">https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3792772</a>
<a href="https://fairware.cs.umass.edu/papers/Verma.pdf">https://fairware.cs.umass.edu/papers/Verma.pdf</a>
<a href="https://www.holisticai.com/blog/holistic-ai-library-tutorial">https://www.holisticai.com/blog/holistic-ai-library-tutorial</a>
<a href="https://arxiv.org/pdf/1801.07593.pdf">https://arxiv.org/pdf/1801.07593.pdf</a></p>
